#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DDPGè®­ç»ƒè„šæœ¬ - é€Ÿåº¦é—­ç¯æ§åˆ¶ç‰ˆ
ç›®æ ‡ï¼šæ§åˆ¶è½¦è¾†åœ¨ä¿æŒæœ€ä½³æ»‘ç§»ç‡ï¼ˆä¸æ‰“æ»‘ï¼‰çš„å‰æä¸‹ï¼Œå°½å¿«åŠ é€Ÿè‡³ç›®æ ‡é€Ÿåº¦å¹¶ä¿æŒç¨³å®šã€‚
"""
import numpy as np
import torch
import os
import time
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

# è¯·ç¡®ä¿è¿™ä¸¤ä¸ªæ¨¡å—è·¯å¾„æ­£ç¡®ï¼Œæˆ–è€…æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´import
from pycarsimlib.rl.ddpg_agent import DDPGAgent
from pycarsimlib.rl.env_carsim_simulink import CarsimSimulinkEnv


def train_ddpg_simulink(
    max_episodes: int = 500,
    max_torque: float = 1000.0,      # æå‡æœ€å¤§æ‰­çŸ©ï¼Œç¡®ä¿åŠ¨åŠ›å……è¶³
    target_slip_ratio: float = 0.1, # å†°é›ªä½é™„ç€è·¯é¢æœ€ä½³æ»‘ç§»ç‡
    target_speed: float = 30.0,     # [æ–°å¢] ç›®æ ‡å·¡èˆªé€Ÿåº¦ (km/h)
    log_dir: str = "logs"
):
    # --- 1. å®šä¹‰å¥–åŠ±å‡½æ•°æƒé‡ (Reward Weights) ---
    # è¿™é‡Œçš„æƒé‡å†³å®šäº†æ™ºèƒ½ä½“çš„å­¦ä¹ æ–¹å‘
    reward_weights = {
            # [æ­£å‘æ¿€åŠ±] 
            'w_tracking': 0,     # é«˜æ–¯æ»¡åˆ† +40 åˆ†ã€‚èµ·æ­¥è¯¯å·®å¤§æ—¶æ¥è¿‘0ï¼Œä½†ä¸æ˜¯è´Ÿæ•°ã€‚
            'w_accel': 150,         
            'w_energy': -10,
            # [çº¦æŸ]
            'w_consistency': -20.0, 
            'w_yaw': -2.0,
            'w_slip': -10.0,        
            'w_smooth': -30.0       
        }
    
    # è®°å½•è¶…å‚æ•°ç”¨äº TensorBoard å±•ç¤º
    hyperparams = {
        'Target Speed (km/h)': target_speed,
        'Max Torque (Nm)': max_torque,
        'Target Slip': target_slip_ratio,
        'Max Episodes': max_episodes,
        'Batch Size': 128,
        'Actor LR': 1e-4,
        'Critic LR': 1e-3,
        'Gamma': 0.99
    }

    # --- 2. åˆå§‹åŒ–æ—¥å¿— ---
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(log_dir, f"speed_tracking_{current_time}")
    os.makedirs(log_path, exist_ok=True)
    writer = SummaryWriter(log_dir=log_path)
    
    # å°†å‚æ•°è¡¨å†™å…¥ TensorBoard çš„ Text é¢æ¿
    md_table = "### Reward Coefficients\n| Key | Value |\n|---|---|\n"
    for k, v in reward_weights.items():
        md_table += f"| {k} | {v} |\n"
    
    md_table += "\n### Hyperparameters\n| Key | Value |\n|---|---|\n"
    for k, v in hyperparams.items():
        md_table += f"| {k} | {v} |\n"
        
    writer.add_text("Configuration/Parameters", md_table, 0)
    
    print(f"è®­ç»ƒæ—¥å¿—å°†ä¿å­˜è‡³: {log_path}")
    print(f"è¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir={log_dir} æŸ¥çœ‹æ›²çº¿")

    # --- 3. åˆå§‹åŒ–ç¯å¢ƒ ---
    env = CarsimSimulinkEnv(
        sim_time_s=20.0,        # [ä¿®æ”¹] å¢åŠ åˆ°20ç§’ï¼Œç»™è¶³æ—¶é—´è¿›å…¥å·¡èˆªçŠ¶æ€
        delta_time_s=0.01,
        max_torque=max_torque,
        target_slip_ratio=target_slip_ratio,
        target_speed=target_speed,
        reward_weights=reward_weights, # ä¼ å…¥æƒé‡å­—å…¸
        send_port=9202,
        recv_port=8087
    )
    
    # è·å–ç»´åº¦
    state_dim = env.get_state_dim()
    action_dim = env.get_action_dim()
    
    # --- 4. åˆå§‹åŒ– Agent ---
    agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        action_bound=1.0,
        hidden_dim=256,
        batch_size=128,
        actor_lr=1e-4,
        critic_lr=1e-3,
        gamma=0.99,
        tau=0.005,
        buffer_capacity=100000,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # è®­ç»ƒå˜é‡
    best_reward = -float('inf')
    noise_scale = 0.5       # åˆå§‹æ¢ç´¢å™ªå£°
    min_noise = 0.05        # æœ€å°å™ªå£°
    noise_decay = 0.95     # å™ªå£°è¡°å‡ç‡
    
    print("\n========== å¼€å§‹è®­ç»ƒ (Speed Tracking Task) ==========")
    print(f"ç›®æ ‡é€Ÿåº¦: {target_speed} km/h | æœ€å¤§æ‰­çŸ©: {max_torque} Nm")
    
    for episode in range(max_episodes):
        # é‡ç½®ç¯å¢ƒ
        state, info = env.reset()
        agent.reset_noise()
        episode_reward = 0
        step_count = 0
        
        # ç»Ÿè®¡æ•°æ®å®¹å™¨
        slip_errors = []
        speed_errors = []
        torque_smoothness = []
        
        start_time = time.time()
        
        while True:
            # 1. é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, noise_scale=noise_scale)
            
            # 2. ç¯å¢ƒäº¤äº’
            next_state, reward, done, info = env.step(action)
            
            # 3. å­˜å‚¨ç»éªŒ
            agent.push(state, action, reward, next_state, done)
            
            # 4. æ¨¡å‹è®­ç»ƒ
            c_loss, a_loss = agent.train_step()
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            step_count += 1
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            if "slip_error" in info: slip_errors.append(info["slip_error"])
            if "speed_error" in info: speed_errors.append(abs(info["speed_error"]))
            if "torque_smoothness" in info: torque_smoothness.append(info["torque_smoothness"])
            
            if done:
                break
        
        # --- Episode ç»“æŸå¤„ç† ---
        duration = time.time() - start_time
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_slip_error = np.mean(slip_errors) if slip_errors else 0.0
        avg_speed_error = np.mean(speed_errors) if speed_errors else 0.0
        avg_smoothness = np.mean(torque_smoothness) if torque_smoothness else 0.0
        final_speed = info.get('vx', 0)
        
        # 1. è®°å½•åˆ° TensorBoard
        writer.add_scalar('Train/Reward', episode_reward, episode)
        writer.add_scalar('Train/Avg_Speed_Error_kmh', avg_speed_error, episode) # å…³æ³¨è¿™ä¸ªæ›²çº¿æ˜¯å¦ä¸‹é™
        writer.add_scalar('Train/Final_Speed_kmh', final_speed, episode)         # å…³æ³¨è¿™ä¸ªæ˜¯å¦è¶‹å‘100
        writer.add_scalar('Train/Avg_Slip_Error', avg_slip_error, episode)
        writer.add_scalar('Train/Torque_Smoothness', avg_smoothness, episode)
        writer.add_scalar('Train/Noise', noise_scale, episode)
        
        if c_loss is not None:
            writer.add_scalar('Loss/Critic', c_loss, episode)
            writer.add_scalar('Loss/Actor', a_loss, episode)
            
        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹
        if episode_reward > best_reward:
            best_reward = episode_reward
            agent.save_model(os.path.join(log_path, "best_model.pt"))
            print(f"ğŸš€ æ–°çºªå½•! Ep {episode+1} Reward: {episode_reward:.1f} (å·²ä¿å­˜)")
            
        # 3. å®šæœŸ Checkpoint
        if (episode + 1) % 50 == 0:
            agent.save_model(os.path.join(log_path, f"checkpoint_ep{episode+1}.pt"))
            
        # 4. å™ªå£°è¡°å‡
        noise_scale = max(min_noise, noise_scale * noise_decay)
        
        # æ‰“å°è¿›åº¦
        print(f"Ep {episode+1}/{max_episodes} | "
              f"Reward: {episode_reward:.1f} | "
              f"EndSpeed: {final_speed:.1f} km/h | "
              f"Err: {avg_speed_error:.1f} | "
              f"Time: {duration:.1f}s")
              
    # è®­ç»ƒç»“æŸ
    agent.save_model(os.path.join(log_path, "final_model.pt"))
    writer.close()
    env.close()
    print("è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    # ç¡®ä¿ logs ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    
    train_ddpg_simulink()